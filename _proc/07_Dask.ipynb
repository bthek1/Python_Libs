{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Dask is a flexible library for parallel computing in Python that can\n",
    "  scale from a single machine to a cluster of machines. It allows you to work with\n",
    "  larger-than-memory datasets and provides parallel execution on task scheduling.\n",
    "  Dask integrates well with Python's existing data science libraries like NumPy, pandas,\n",
    "  and Scikit-Learn\n",
    "output-file: dask.html\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "title: Dask\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca6e42-5888-470b-8a7f-474fd39dae8a",
   "metadata": {},
   "source": [
    "## Key Concepts and Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e88c54-eb0e-4231-828d-a3ec47b132ca",
   "metadata": {},
   "source": [
    "a. Dask Arrays\n",
    "\n",
    "> Dask arrays extend NumPy arrays to support larger-than-memory computations by splitting the array into many smaller chunks, each one a NumPy array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb2bfa-7d2a-489f-9990-d50498d994c8",
   "metadata": {},
   "source": [
    "b. Dask DataFrames\n",
    "\n",
    "> Dask DataFrames extend pandas DataFrames for parallel and distributed computing. They are composed of many smaller pandas DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ce6c7-a162-49bb-8ba7-0f9165269592",
   "metadata": {},
   "source": [
    "c. Dask Bags\n",
    "\n",
    "> Dask Bags are like parallel lists that provide map, filter, and groupby operations on potentially larger-than-memory datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4547a18-e175-42d1-9bba-cfc7ad881538",
   "metadata": {},
   "source": [
    "d. Dask Delayed\n",
    "\n",
    "> Dask Delayed allows you to build task graphs in a low-level way, specifying operations and dependencies between them. It's useful for custom workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8297d8c-57e9-49d9-9233-7c87cb9dcbdf",
   "metadata": {},
   "source": [
    "e. Dask Distributed\n",
    "\n",
    "> Dask Distributed is a distributed computing framework that scales Dask workflows to a cluster of machines. It includes a task scheduler and workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cfb9e1-4d9c-45c1-a8ad-5a3b7dbbda4f",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf1997-a00a-4939-bbeb-4363817edc0c",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install dask[complete]  # Installs core Dask and extras\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94cae4-c24b-48f6-8a5a-335aad8b1d80",
   "metadata": {},
   "source": [
    "> For the distributed scheduler, you might need\n",
    "\n",
    "```sh\n",
    "pip install dask distributed\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed83ab-2dc1-4e12-a8af-41fa7397067a",
   "metadata": {},
   "source": [
    "## Dask Arrays\n",
    "\n",
    "> Dask arrays work similarly to NumPy arrays but can operate on data too large to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31fb7d-190e-4b33-95e3-6ebad2d454d4",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# Create a large Dask array with chunks\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "\n",
    "# Compute the sum along an axis\n",
    "result = x.mean(axis=0)\n",
    "\n",
    "# Trigger computation\n",
    "result = result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c1563-d035-4cc1-86d3-15f75576dbf8",
   "metadata": {},
   "source": [
    "## Dask DataFrames\n",
    "\n",
    "> Dask DataFrames are similar to pandas DataFrames but designed for parallel processing on larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822d8f8-aab8-4a9c-aaa2-b6ea8fc46ca1",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/envs/pfast/lib/python3.12/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "new = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9ccda-e7b2-4f70-a5d4-487179069084",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "dask_df = dd.from_pandas(new, npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0066218-c41b-4ee6-8a72-071c9308d32d",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.752109\n",
       "1       0.498086\n",
       "2       0.500595\n",
       "3       0.501720\n",
       "4       0.501788\n",
       "          ...   \n",
       "9995    0.503008\n",
       "9996    0.497020\n",
       "9997    0.498570\n",
       "9998    0.494503\n",
       "9999    0.496511\n",
       "Length: 10000, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a large CSV file into a Dask DataFrame\n",
    "df = dask_df\n",
    "\n",
    "# Perform some operations\n",
    "result = df[df[0] > 0.5].mean()\n",
    "\n",
    "# Compute the result\n",
    "result = result.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278bb07b-81e8-4980-9386-df9f02d83695",
   "metadata": {},
   "source": [
    "## Dask Bags\n",
    "\n",
    "> Dask Bags are useful for working with unstructured or semi-structured data, such as text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad15f9-528e-41b8-b039-a6a4277512a4",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "# Create a Dask Bag from a list\n",
    "data = db.from_sequence([1, 2, 3, 4, 5])\n",
    "\n",
    "# Apply a function to each element\n",
    "squares = data.map(lambda x: x ** 2)\n",
    "\n",
    "# Compute the result\n",
    "result = squares.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eed7f3-a7e3-41d1-8798-01572adfc0e4",
   "metadata": {},
   "source": [
    "## Dask Delayed\n",
    "\n",
    "> Dask Delayed allows for parallel execution by specifying the dependencies between tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ead9d-2db4-423f-880d-1802fb00e956",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delayed('sum_list-bf70cda4-342a-46f4-8ebe-d863e28fea4e')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask import delayed\n",
    "\n",
    "@delayed\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "@delayed\n",
    "def sum_list(lst):\n",
    "    return sum(lst)\n",
    "\n",
    "# Define a computation graph\n",
    "x = add(1, 2)\n",
    "y = add(3, 4)\n",
    "total = sum_list([x, y])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488512a-d037-4c0c-ab78-9d9ba28abf84",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Compute the result\n",
    "result = total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d09a9-7877-4616-b7e8-f4932e5ec568",
   "metadata": {},
   "source": [
    "## Dask Distributed\n",
    "\n",
    "> Dask Distributed allows scaling computations to multiple machines. Start a local cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a407b-befe-401e-98e9-726660f24496",
   "metadata": {},
   "source": [
    "```python\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Start a local cluster\n",
    "client = Client()\n",
    "\n",
    "# Check cluster status\n",
    "print(client)\n",
    "\n",
    "# Example with Dask DataFrame\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Read a CSV file and perform operations\n",
    "df = dask_df\n",
    "result = df.groupby(0).mean().compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55dbec-e3ed-4aa6-917b-0beaf73c8224",
   "metadata": {},
   "source": [
    "## Monitoring and Debugging\n",
    "\n",
    "> Dask provides several tools for monitoring and debugging:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcc7d5-9b31-456c-90db-471a51681acb",
   "metadata": {},
   "source": [
    "- Dask Dashboard: A web-based interface that shows the status of computations and the Dask cluster.\n",
    "- Visualizing Task Graphs: Use dask.visualize to visualize task graphs and understand dependencies.\n",
    "- Logs and Errors: Dask provides detailed logging to help identify and fix issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b71142-c730-442e-ac58-87c3eb0cf66c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
